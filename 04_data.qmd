# Data

## Data extraction and sample construction

I collect daily cryptocurrency data on open, high, close, and low (OHCL) prices, 24-hour volume, and market capitalization (calculated as the cryptocurrency's USD price multiplied by its circulating supply) from [CoinCodex](https://coincodex.com/), a website-data provider that gathers and aggregates data from more than 400 exchanges. I extract the data, all expressed in US dollars, using the CoinCodex API as follows: 

1. I retrieve the list of all available cryptocurrencies and extract each cryptocurrency shortname, also referred to as the "slug". At the time of writing, there are 14,907 unique cryptocurrency shortnames listed in the API. 

2. Using the slug, I construct an URL for each cryptocurrency to obtain the metadata from the API. I parse the JSON API response into a dataframe and extract the OHCL prices, volume, and market capitalization daily data. I exclude those observations with non-zero or missing values in any of these fields.  

Out of the 14,907 cryptocurrencies listed, only 7,272 entries contained available data. Next, following the methodology of @bianchiMispricingRiskCompensation2021 and @mercikCrosssectionalInteractionsCryptocurrency2025, I apply a series of cleaning and filtering steps in order to remove possible innacuracies in the dataset: 

1. Non-positive and missing values. As mentioned earlier, I remove observations with where prices, volume, or market capitalization were non-positive or missing. 
2. Small cryptocurrencies. Similar to @liuCommonRisk2022, I screen out small cryptocurrencies and consider only those with a market capitalization greater than one million USD. Therefore, I exclude observations for coins whose market capitalization falls below this minimum threshold, which allows for the possibility that a coin may become "small" after a certain period or event. 
3. Cryptocurrency type. Based on the cryptocurrency classification from [CoinMarketCap](https://coinmarketcap.com/cryptocurrency-category/) and CoinCodex, I exclude: 

    - stablecoins.  I include (i) centralized stablecoins, which are backed and pegged to fiat currency or physical assets by a third party, such as Tether (USDT), USD Coin (USDC), and Euro Coin (EURC), and (ii) algorithmically stabilized stablecoins, which use algorithms to adjust the circulating supply in response to changes in demand to maintain a stable value with the underlying asset, such as DAI and AMPL [FSB, -@financialstabilityboardAddressingRegulatory2020].

    - wrapped cryptocurrency tokens, which mirror the value of another cryptocurrency from a different blockchain, e.g., Wrapped Bitcoin (wBTC) or Wrapped Ethereum (wETH) [@coinbaseWhatWrapped]. 
    - cryptocurrencies backed by or pegged to gold or precious metals, including Pax Gold (PAXG) or XAGx Silver Token (XAGX).
    
4. Volume-to-market-capitalization ratio. To filter out cryptocurrencies with "fake" or "erroneous" trading volume, I calculate the daily volume-to-market-capitalization ratio for each token and exclude observations where the ratio exceeds 1.
5. Extreme returns. To minimize the influence of extreme values in my results, I winsorize daily cryptocurrency returns to lie within the range of -90% to 500%.
6. Time period. Even though cryptocurrency data are available since 2014, I use data from June 1, 2018 for the empirical analysis due to the low amount of coins available before this date (see @fig-numcoins). 
7. 6. Minimum observations. In order to maintain practical relevance, I keep cryptocurrencies that have at least 365 consecutive daily observations and those with at least 730 observations in the complete panel of coin characteristics (see @sec-characteristics), which is equivalent to 2 years of historical data. Therefore, I exclude very short-lived coins, but retain failed coins with this relatively large number of observations, which help to lessen the so called "survivorship biais".


::: {#fig-numcoins fig-scap="Number of cryptocurrencies over time"}
<!-- fig-scap="short subcaption 1"  Use this to add a title in LOF for each subplot-->
![](pictures/timeseries_daily_coins.png){#fig-sub1 width=80%}

![](pictures/coins_per_year.png){#fig-sub2 width=80%} 

**Number of cryptocurrencies over time.** Panel A shows the daily time series of the number of unique cryptocurrencies. Panel B displays the number of unique cryptocurrencies recorded each year. Both panels correspond to the dataset after applying the filtering steps (1) to (5), covering the period from January 1, 2014, to July 31, 2025, and including 1,416 unique cryptocurrencies.  Note that coins may enter or exit the market over time. 
:::

## Sample overview

After applying all the filters, the resulting sample consists of 973 unique cryptocurrencies and 1,478,936 observations from June 1, 2018, to July 31, 2025, where a day starts at 00:00:00 UTC. It is important to mention that the number of cryptocurrencies fluctuates over the entire period, which results in an unbalanced panel of data. @tbl-sampleoverview provides an overview of the descriptive statistics for the cryptocurrency excess returns. Additionally,  


:::{#tbl-sampleoverview tbl-scap="Summary statistics of daily excess returns"}

```{r}
#| echo: false
#| warning: false
##| label: tbl-sampleoverview
##| tbl-subcap: "**Summary statistics of daily excess returns.** The table reports the summary statistics of daily returns on the dataset used in the empirical analysis. The columns represent the number of daily observations, the quantity of unique coins over the whole sample period, the minimum size of the cross-section, the mean of the excess returns, the standard deviation, and the 10th percentile, lower quartile, median, upper quartile, and 90th percentile of the distribution of the returns. The sample period is from June 1, 2018, to July 31, 2025."
##| tbl-scap: "Summary statistics of daily excess returns"
library(readr)
library(dplyr)
library(gt)

tbl <- read_csv("tables/sample_overview_table.csv", 
                show_col_types = FALSE) |>
  select(-...1)

gt_tbl <- tbl |>
  gt(rowname_col = NULL) |>
  fmt_percent(columns = c(Mean, Std, P10, P25, P50, P75, P90), decimals = 2) |>
  cols_align(
    align = "center",
    columns = everything()
  ) |>
  tab_options(table.font.size = px(12),
              footnotes.padding = px(20)) |>
  tab_footnote(
    footnote = "Some text for the footnote"
  ) |>
  as_latex()

gt_tbl
```
**Summary statistics of daily excess returns.** 
:::


**SECOND TRY** 

```{r}
#| echo: false
#| warning: false
library(tibble)
library(gt)
library(dplyr)

# Original table
results_table <- tribble(
  ~Metric, ~Intercept, ~`K = 3`, ~`K = 5`, ~`K = 8`,

  "\\(R^2_{\\text{Total}}\\)", "\\(\\Gamma_\\alpha = 0\\)", 0.2301, 0.2509, 0.2681,
  "",                          "\\(\\Gamma_\\alpha \\neq 0\\)", 0.2322, 0.2524, 0.2690,
  "\\(R^2_{\\text{Predictive}}\\)", "\\(\\Gamma_\\alpha = 0\\)", -0.3904, -0.4082, -0.4169,
  "",                              "\\(\\Gamma_\\alpha \\neq 0\\)", -0.3857, -0.4055, -0.4156,

  "\\(R^2_{\\text{Total}}\\)", "\\(\\Gamma_\\alpha = 0\\)", 0.2625, 0.2817, 0.2934,
  "",                          "\\(\\Gamma_\\alpha \\neq 0\\)", 0.2661, 0.2826, 0.2937,
  "\\(R^2_{\\text{Predictive}}\\)", "\\(\\Gamma_\\alpha = 0\\)", 0.1725, 0.1551, 0.1511,
  "",                              "\\(\\Gamma_\\alpha \\neq 0\\)", 0.1719, 0.1584, 0.1554,

  "\\(R^2_{\\text{Total}}\\)", "--", 1.455576e-05, 2.478456e-05, 2.934438e-05,
  "\\(R^2_{\\text{Predictive }}\\)", "", 1.448428e-05, 2.469351e-05, 2.923816e-05
)

# Make gt table
gt_tbl <- results_table |>
  gt() |>
  fmt_number(
    columns = c(`K = 3`, `K = 5`, `K = 8`),
    decimals = 4
  ) |>
  cols_label(
    Metric = "",
    Intercept = "",
    `K = 3` = html("\\(K = 3\\)"),
    `K = 5` = html("\\(K = 5\\)"),
    `K = 8` = html("\\(K = 8\\)")
  ) |>
  tab_spanner_delim(delim = "=") |> # optional for grouping
  tab_row_group(
    group = "Panel A: IPCA on daily data",
    rows = 1:4
  ) |>
  tab_row_group(
    group = "Panel B: IPCA on weekly data",
    rows = 5:8
  ) |>
  tab_row_group(
    group = "Panel C: PCA on weekly data",
    rows = 9:10
  ) |>
  cols_align(align = "center", columns = everything()) |>
  tab_options(
    table.font.size = px(12)
  ) 

```

<!-- Passing raw_latex -->
<!-- ```{r,  echo=FALSE} -->
<!-- #| output: asis -->
<!-- library(gt) -->
<!-- ltx <- as.character(gt::as_latex(gt_tbl)) -->

<!-- # Wrap the LaTeX code in Pandoc's raw latex block -->
<!-- ltx_block <- paste0("```{=latex}\n", ltx, "\n```") -->

<!-- knitr::asis_output(ltx_block) -->
<!-- ``` -->


```{=latex}
\begin{table}[t]
\fontsize{9.0pt}{10.8pt}\selectfont
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}ccccc}
\toprule
 &  & \multicolumn{3}{c}{K } \\
\cmidrule(lr){3-5}
 &  & \(K = 3\) & \(K = 5\) & \(K = 8\) \\
\midrule\addlinespace[2.5pt]
\multicolumn{5}{l}{Panel C: PCA on weekly data} \\[2.5pt]
\midrule\addlinespace[2.5pt]
\(R^{2}_{\text{Total}}\) &  & 0.0000 & 0.0000 & 0.0000 \\
\(R^{2}_{\text{Predictive}}\) &  & 0.0000 & 0.0000 & 0.0000 \\
\midrule\addlinespace[2.5pt]
\multicolumn{5}{l}{Panel B: IPCA on weekly data} \\[2.5pt]
\midrule\addlinespace[2.5pt]
\(R^{2}_{\text{Total}}\) & \(\Gamma_{\alpha} = 0\) & 0.2625 & 0.2817 & 0.2934 \\
 & \(\Gamma_{\alpha} \neq 0\) & 0.2661 & 0.2826 & 0.2937 \\
\(R^{2}_{\text{Predictive}}\) & \(\Gamma_{\alpha} = 0\) & 0.1725 & 0.1551 & 0.1511 \\
 & \(\Gamma_{\alpha} \neq 0\) & 0.1719 & 0.1584 & 0.1554 \\
\midrule\addlinespace[2.5pt]
\multicolumn{5}{l}{Panel A: IPCA on daily data} \\[2.5pt]
\midrule\addlinespace[2.5pt]
\(R^{2}_{\text{Total}}\) & \(\Gamma_{\alpha} = 0\) & 0.2301 & 0.2509 & 0.2681 \\
 & \(\Gamma_{\alpha} \neq 0\) & 0.2322 & 0.2524 & 0.2690 \\
\(R^{2}_{\text{Predictive}}\) & \(\Gamma_{\alpha} = 0\) & -0.3904 & -0.4082 & -0.4169 \\
 & \(\Gamma_{\alpha} \neq 0\) & -0.3857 & -0.4055 & -0.4156 \\
\bottomrule
\end{tabular*}
\end{table}
```

```{=latex}
\begin{tabular}{lrrrrrrrrrr}
\toprule
No. Obs & Unique coins & Min No. Obs & Mean & Std & P10 & P25 & P50 & P75 & P90 \\
\midrule
1478936 & 973 & 121 & -0.027021 & 0.124855 & -0.101770 & -0.066473 & -0.037020 & 0.000195 & 0.046910 \\
\bottomrule
\end{tabular}
```


**TEST**

```{r}
tab_latex <-
  gtcars |>
  dplyr::select(mfr, model, msrp) |>
  dplyr::slice(1:5) |>
  gt() |>
  tab_header(
    title = md("Data listing from **gtcars**"),
    subtitle = md("`gtcars` is an R dataset")
  ) |>
  as_latex()

tab_latex

```

**ANOTHER TEST**

```{r}
#| echo: false
#| warning: false
library(tibble)
library(knitr)
library(kableExtra)
library(latex2exp)

df <- read_csv("tables/sample_overview_table.csv", show_col_types = FALSE) |>
  as.data.frame()

kable(df, "latex", align="c", booktabs=TRUE, escape = , caption = '$\\Gamma$') %>%
footnote(general=c("$a^2+b^2=c^2,$",     
                   "$\\\\sigma^2=\\\\frac{1}{n-1}\\\\sum_{i=1}^n(x_i-\\\\bar{x})^2;$", 
                   "1,000 \\\\$;", "100\\\\%."),
         number=c("Hello\ there! \\\\textit{Hello\ there!}"),
         footnote_as_chunk=TRUE, 
         escape=FALSE)

# kable(df, "latex", align = "c", booktabs = TRUE, escape = F) %>%  # <- TRUE here
#   footnote(
#     general = c("The table reports the summary statistics of daily returns ... July 31, 2025."),
#     number  = c("Hello\ there! \\\\textit{Hello\ there!}"),
#     footnote_as_chunk = TRUE,
#     escape = FALSE   # <- still allow LaTeX in the footnote
#   )

```

**THIS WORKS**

```{r tab}
library(knitr)
library(kableExtra)
df <- data.frame(v1=rnorm(6), v2=runif(6), v3=rbinom(6, 1, .33), 
             row.names=LETTERS[1:6])

df$v4 <- c('My formula $\\sum_{i=1}^9$')
kable(df, "latex", align="c", booktabs=TRUE, escape = F, caption = '$\\Gamma$') %>%
footnote(general=c("$a^2+b^2=c^2,$",     
                   "$\\\\sigma^2=\\\\frac{1}{n-1}\\\\sum_{i=1}^n(x_i-\\\\bar{x})^2;$", 
                   "1,000 \\\\$;", "100\\\\%."),
         number=c("Hello\ there! \\\\textit{Hello\ there!}"),
         footnote_as_chunk=TRUE, 
         escape=FALSE)
```

## Characteristic construction and description {#sec-characteristics}

### Volume shock

Following Bianchi et al. (2022), the volume shock is defined as the log-deviation of trading volume from its rolling average (over 30 or 60 days) for cryptocurrency $i$ at time $t$. For $m \in \{30, 60\}$ periods, the volume shock is estimated as:

$$
v_{i,t} = \log(\text{Volume}_{i,t}) - \log\left( \frac{1}{m} \sum_{s=1}^{m} \text{Volume}_{i,t-s} \right)
$$

## Risk

### Realized volatility (rvol)

Using the volatility estimator of Yang and Zhang (2000), I compute the daily realized volatility based on OHCL prices over a rolling 30-day window. For $n > 1$ number of periods, the volatility estimate at time $t$ is:  

$$
\sigma_t = \sqrt{\sigma^2_O + k\sigma^2_C + (1 - k)\sigma^2_{RS}}
$$
where $\sigma^2_{RS}$ is the variance estimator of Rogers et al. (1994), and $\sigma^2_O$, $\sigma^2_C$, $k$ are defined as follows: 

$$
\sigma^2_O = \frac{1}{n-1}\sum\limits_{i=1}^n(o_i - \bar o)^2,
$$

$$
\sigma^2_C = \frac{1}{n-1}\sum\limits_{i=1}^n(c_i - \bar c)^2,
$$

$$
k = \frac{\alpha -1}{\alpha + \frac{n+1}{n-1}}
$$

with $o = ln\,O_t - ln\,C_{t-1}$, and $c = ln\,C_t - ln\,O_t$. Here, $C_{t-1}$ denotes the last days' closing price and $O_t$ the current day's opening price. I set the constant $\alpha = 1.34$ as suggested by Yang and Zhang (2000) to be the best value in practice.  



@moskowitzTimeSeriesMomentum2012




This is more related to factor construction. 

Organize week in the following way: the first seven days of the year forms the first week, and the first 51 weeks of the year consists of 7 days each. The 52th week of the year consists of the last eight days and, in case of a leap year (as 2016, 2020, and 2024), of nine days. 

Similar to **Liu et al**, I construct a daily cryptocurrency market return as the value-weighted average return of all the cryptocurrencies in the sample. For cryptocurrencies $i = 1, ..., N$, the daily market return at time $t$ is computed as: 

$$
r_t^M = \frac{\sum_{i=1}^N r_{it} \cdot marketcap_{it}}
             {\sum_{i=1}^N marketcap_{it} }
$$

The cryptocurrency market excess return (`CMKT`) is constructed as the difference between the cryptocurrency market return and the risk-free rate. To proxy the risk-free rate, I used the (daily) 1-month Treasury bill rate from the FRED. 



Write this in the following section of "Empirical application" or 
This is for the model:
7. (Still undecisive) Minimum cross-section. Following the criterion by Kelly, I 
Convert variables in the -0.5 - 0.5 range
    
The sample period ranges from January 1st, 2014, to May 31st, 2025.  
